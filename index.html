<!doctype html>
<html lang="en">
<head>
<title>Image Super Resolution using Hybrid Attention Transformer</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<style>
    .image-container {
  	display: flex;
	justify-content: space-between;
      flex-wrap: wrap;
      max-width: 800px;
    }

    .image-box {
      box-sizing: border-box;
      margin-bottom: 20px;
    }

    .image-box img {
      border-radius: 8px;
    }

    .text-box {
      text-align: center;
    }

	#section1, #section2, #section3, #section4, #section5, #section6, #section7, #section8 {
      margin-bottom: 100px; 
    }
  </style>
</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Image Super Resolution using Hybrid Attention Transformer</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</h2>
</div>
</div>
<div class="row">
<div class="col">

<h2>Table of Contents</h2>
<ul>
  <li><a href="#section1">Paper Overview</a></li>
  <li><a href="#section2">Literature Review</a></li>
  <li><a href="#section3">Biography</a></li>
<li><a href="#section4">Social Impact</a></li>
	<li><a href="#section5">Industry Applications</a></li>
	<li><a href="#section6">Follow-on Research</a></li>
	<li><a href="#section7">Peer Review</a></li>
	<li><a href="#section8">References</a></li>
</ul>

<section id="section1">
<h2>Paper Overview</h2>
<h5><b>Problem: Image Super Resolution</b></h5>
 <p>Single image super-resolution is a classic and challenging problem in computer vision. It has practical applications in various domains, including medical imaging, satellite imagery, and enhancing the quality of digital photographs.<br>
	 We input a low-resolution image and ask the network to generate a higher-resolution image.</p>
	<div class="image-container">
  <div class="image-box">
    <img height="203" src="./imgs/LowRes.png" width="170">
    <div class="text-box">
      <p>Low-Resolution Image</p>
    </div>
  </div>
	
  <div class="image-box">
    <img height="300" src="./imgs/TraditionalInterpolation.png" width="250">
    <div class="text-box">
      <p>Traditional Interpolated Image</p>
    </div>
  </div>

  <div class="image-box">
    <img height="300" src="./imgs/SR.png" width="250">
    <div class="text-box">
      <p>AI Super Resolution Image</p>
    </div>
  </div>
</div>

<h5><b>Solution: Hybrid Attention Transformer (HAT)</b></h5>
<p> CNN-based methods have long dominated the Image super-resolution (SR) field. Recently Transformers have attracted attention. SwinIR a transformer-based model, obtained a breakthrough.
Despite the success, "Why Transformer is better than CNN" remains a mystery. An intuitive explanation is that transformers can benefit from the self-attention mechanism and utilize long-range information. Interestingly, authors find that SwinIR does NOT exploit more input pixels than CNN-based methods. The proposed HAT (Hybrid Attention Transformer achieves higher pixel activation as shown below.</p>
<img height="350" src="./imgs/LAM.png" width="700">
<br><br>

<h5><b>Novel Contributions</b></h5>
<ul>
  <li>Authors have designed a novel Hybrid Attention Transformer (HAT) that combines self-attention, channel attention and a new overlapping cross-attention to activate more pixels for better reconstruction</li>
  <li>Authors propose an effective same-task pre-training strategy to further exploit the potential of SR Transformer and show the importance of large-scale data pre-training for the task</li>
  <li>Method achieves state-of-the-art performance</li>
</ul>
<img height="500" src="./imgs/Architecture.png" width="1000">
<br><br>

<h5><b>Results</b></h5>

<img height="400" src="./imgs/Result3.png" width="1000">
<img height="400" src="./imgs/Result5.png" width="1000">
<img height="400" src="./imgs/Result6.png" width="1000">

</section>

<section id="section2">
<h2>Literature Review</h2>
<h4>Historical Background of </h4>
<p><b>Evolution of ISR:</b> The journey of image super-resolution (ISR) has evolved from basic image enhancement techniques to the use of advanced deep learning models. Particularly, Convolutional Neural Networks (CNNs) have played a pivotal role in addressing resolution limitations across various fields.</p>
<p><b>Transformers in Vision:</b> Building on the success in natural language processing, Transformers have been adopted into the realm of computer vision. This transition brought significant improvements in capturing long-term dependencies and processing global information in ISR tasks.</p>
<p><b>Development of HAT:</b> The paper introduces the Hybrid Attention Transformer (HAT), a novel approach that synergizes channel and self-attention mechanisms. This development aims to overcome previous limitations of Transformers in ISR, enabling optimized utilization of input information for enhanced image reconstruction.</p>
<div class="row">
    <div class="col-md-6">
        <img height="350" src="./imgs/cite2.png" width="500">
    </div>
    <div class="col-md-6">
        <img height="350" src="./imgs/cite.png" width="500">
    </div>
</div>
<p>* SwinIR: This is a landmark Transformer-based network that achieved breakthrough improvement in SR, but it had limitations in terms of the range of information utilized for reconstruction​​ </p>
</section>


<section id="section3">
<h2>Biography</h2>

<div class="author-bio">
  <h3>Xiangyu Chen</h3>
  <img height="100" src="https://chxy95.github.io/images/xy1.jpg" alt="Xiangyu Chen">
  <p>Xiangyu Chen, currently a joint Ph.D. student at the University of Macau and Shenzhen Institute of Advanced Technology, specializes in computer vision and computational photography with a focus on image super-resolution and general image restoration.</p>
</div>

<div class="author-bio">
  <h3>Xintao Wang</h3>
  <img height="100" src="./imgs/Xintao.png" alt="Xintao Wang">
  <p>Xintao Wang, a senior staff researcher at Tencent ARC Lab, leads efforts in visual content generation. He completed his Ph.D. at the Chinese University of Hong Kong and has made significant contributions to the field of image and video generation/editing.</p>
</div>

<div class="author-bio">
  <h3>Jiantao Zhou</h3>
  <img height="100" src="https://www.fst.um.edu.mo/image/staff-photo/jtzhou.jpg" alt="Jiantao Zhou">
  <p>Professor Jiantao Zhou, from the University of Macau, PhD in ECE, Hong Kong University of Science and Technology</p>
</div>

<div class="author-bio">
  <h3>Yu Qiao</h3>
  <img height="100" src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=gFtI-8QAAAAJ&citpid=1" alt="Yu Qiao">
  <p>Professor of Shanghai AI Laboratory; Shenzhen Institutes of Advanced Technology, Ph.D. in Mechanical Engineering @ MIT</p>
</div>

<div class="author-bio">
  <h3>Chao Dong</h3>
  <img height="100" src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=OSDCB0UAAAAJ&citpid=3" alt="Chao Dong">
  <p>Full Professor	Shenzhen Institutes of Advanced Technology	 
     PhD @ The Chinese University of Hong Kong</p>
</div>

	<ul><li>Xiangyu Chen and Jiantao Zhou are with State Key Laboratory of Internet
of Things for Smart City, University of Macau.</li>
	<li>Xiangyu Chen, Xiangtao Kong, Chao Dong and Yu Qiao are with Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
Shenzhen, China</li>
	<li>Xiangyu Chen, Wenlong Zhang, Xiangtao Kong, Chao Dong and Yu Qiao
are with Shanghai Artificial Intelligence Laboratory, Shanghai, China</li>
	<li>Xintao Wang is with the Applied Research Center, Tencent PCG, Shenzhen, China</li></ul>
</section>
	

<section id="section4">
<h2>Social Impact</h2>
<p>Since the HAT approach advances the state of the art in the Image Super-Resolution field it presents itself as the front-runner candidate to be used in Security and Surevellience products. As we know a deep learning model often hallucinates to generate additional details in the image, extra care should be taken to mitigate model biases. <br>
Similarly in the field of Medicine, care should be taken to make the Super Resolution model more reliable and only use it where hallucination is acceptable.
</p>
</section>

<section id="section5">
<h2>Industry Applications</h2>
<p>HAT promises SOTA performance and has established itself as an industry-ready model. <br>
This paper was supported by Tencent ARC lab, an active player in the super-resolution field. Their use case of the technology lies in their cloud and social media services. Cloud services benefit from super-resolution as it helps upscale low-resolution images in real-time, saving the space that a high-resolution counterpart would have otherwise occupied.<br>
Hence HAT provides companies like Tencent with an improved approach for implementing super resolution onto their products and services.
</p>
</section>

<section id="section6">
<h2>Follow-on Research</h2>
<p> This paper introduces the idea of activating more pixels to get better image reconstructions. Further research could be done to take this idea forward and produce even better-looking image reconstructions. 

<div class="image-container">
  <div class="image-box">
    <img height="400" src="./imgs/HATFIR_architecture.png" width="400">
    <div class="text-box">
      <p>(a) Residual Hybrid Attention Group (RHAG) in the HAT. <br>
	      (b) Swin Fourier Transformer Block (SFTB) in HAT FIR. 
	      <br>The convolution (3x3) in RHAG of HAT is replaced 
	      <br> with SFB to get the SFTB.</p>
    </div>
  </div>
	
  <div class="image-box">
    <img height="400" src="./imgs/HATFIR_table.png" width="400">
    <div class="text-box">
      <p>Quantitative comparison with HAT</p>
    </div>
  </div>
</div>
	
	A very recent paper by D. Zhang et al named "SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution" aims to do this. The propose SwinFIR and HATFIR by replacing Fast Fourier Convolution (FFC) components, which have the image-wide receptive field leading to further improvement in the efficiency of capturing global
information. 
<p>Some papers might contribute to advancing the state of the art in image super-resolution, either by proposing new Transformer architectures, introducing alternative neural network designs, or applying the Transformer model to new types of images and challenges. They all follow up on the HAT paper by exploring different facets of the problem space and proposing innovative solutions that could potentially enhance or improve upon the results obtained by the Hybrid Attention Transformer.</p>
<div class="row">
  <div class="col-md-6">
    <!-- This is where your image would go -->
    <img height="350" src="./imgs/follow.png" width="400">
  </div>
  <div class="col-md-6">
    <p><strong>CSWin Transformer (Dong, 2022):</strong> Introduces a cross-shaped window Transformer for improved efficiency in image tasks.</p>
    <p><strong>MAXIM (Tu, 2022):</strong> Proposes a multi-axis MLP for image processing as an alternative to attention-based methods.</p>
    <p><strong>OSRT (Yu, 2023):</strong> Adapts Transformer models for omnidirectional image super-resolution, addressing image distortions.</p>
  </div>
</div>
</section>


<section id="section7">
<h2>Peer Review</h2>
<h4>Reviewer 1 - Wenyu Zhang </h4>
<div class="peer-review">
    <p><strong>Rate:</strong> 7/10</p>
    <p><strong>Strengths:</strong> Demonstrates significant improvement over existing methods, with extensive experimental validation.</p>
    <p><strong>Weaknesses:</strong> Future work could benefit from a discussion on model complexity and a broader comparative analysis.</p>
    <p><strong>Overall:</strong> Offers a substantial contribution to Transformer applications in super-resolution, meriting publication with minor revisions suggested.</p>
</div>

<h4>Reviewer 2 - Varun Mohan </h4>
<div class="peer-review">
    <p><strong>Rate:</strong> 8/10 (Strong Accept)</p>
    <p><strong>Strengths:</strong> 
	<ul>
  	<li>Achieved SOTA on several test datasets</li>
  	<li>Novel method to activate more pixels</li>
  	<li>Well-written paper and good use of figures</li>
	<li>Excellent citation of previous work</li>
	</ul></p>
    <p><strong>Weaknesses:</strong> 
	<ul>
  	<li>HAT-L variant has a considerably higher model complexity</li>
  	<li>Requires a lot of resources to pre-train and finetune the model</li>
	</ul></p>
    <p><strong>Overall:</strong> Establishes a new SOTA in Image Super-Resolution field, but fails to improve on the model complexity </p>
</div>
</section>


<section id="section8">
<h3>References</h3>

<p><a name="Activating More Pixels in Image Super-Resolution Transformer">[1]</a> <a href="https://arxiv.org/pdf/2205.04437.pdf"
  >Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong.
  <em>"Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</em></a>
</p>
<p><a name="Super Resolution on Arm NPU">[2]</a> <a href="https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/super-resolution-on-arm-npu"
  >Alex Shang, Yabin Zheng, Mary Bennion, and Alex Avramenko.
  <em>Super Resolution on Arm NPU</em></a>
</p>
<p><a name="Super-Resolution from a Single Image">[3]</a> <a href="https://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf"
  >Daniel Glasner, Shai Bagon, Michal Irani.
  <em>"Super-Resolution from a Single Image", ICCV, 2009</em></a>
</p>
<p><a name="Learning a Deep Convolutional Network for Image Super-Resolution">[4]</a> <a href="https://arxiv.org/pdf/1501.00092.pdf"
  >Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.
  <em>"Learning a Deep Convolutional Network for Image Super-Resolution", ECCV, 2014</em></a>
</p>
<p><a name="SwinIR: Image Restoration Using Swin Transformer">[5]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf"
  >Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte.
  <em>"SwinIR: Image Restoration Using Swin Transformer", ICCV 2021</em></a>
</p>
<p><a name="Image super-resolution using very deep residual channel attention networks">[6]</a> <a href="https://arxiv.org/pdf/1807.02758.pdf"
  >Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
  <em>"Image super-resolution using very deep residual channel attention networks", In Proceedings of the European Conference on computer vision (ECCV), pages 286-301, 2018., ICCV 2021</em></a>
</p>
<p><a name="Interpreting super-resolution networks with local attribution maps">[7]</a> <a href="https://arxiv.org/pdf/2011.11036.pdf"
  >Jinjin Gu and Chao Dong.
  <em> "Interpreting super-resolution networks with local attribution maps", In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9199-9208, 2021.</em></a>
</p>
	  
</section>
	


<h2>Team Members</h2>
                                                   
<p>Varun Mohan (mohan.va@northeastern.edu) and Wenyu Zhang (zhang.wenyu1@northeastern.edu)</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
