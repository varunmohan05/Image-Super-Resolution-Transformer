<!doctype html>
<html lang="en">
<head>
<title>Hybrid Attention Transformer for Image Super Resolution</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<style>
    .image-container {
  	display: flex;
      flex-wrap: wrap;
      justify-content: space-between;
      max-width: 800px;
    }

    .image-box {
      flex: 0 0 calc(33.33% - 20px);
      box-sizing: border-box;
      margin-bottom: 20px;
    }

    .image-box img {
      border-radius: 8px;
    }

    .text-box {
      text-align: left;
    }
  </style>
</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Hybrid Attention Transformer for Image Super Resolution</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Activating More Pixels in Image Super-Resolution Transformer</h2>
<p>Paper Overview and Interesting Details</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Paper Overview</h2>
<h5><b>Problem: Image Super Resolution</b></h5>
 <p>We input a low-resolution image and ask the network to generate a higher-resolution image.</p>
	<div class="image-container">
  <div class="image-box">
    <img height="203" src="./imgs/LowRes.png" width="170">
    <div class="text-box">
      <p>Input Low-Resolution Image</p>
    </div>
  </div>

  <div class="image-box">
    <img height="300" src="./imgs/TraditionalInterpolation.png" width="250">
    <div class="text-box">
      <p>Output Traditional Interpolated Image</p>
    </div>
  </div>

  <div class="image-box">
    <img height="300" src="./imgs/SR.png" width="250">
    <div class="text-box">
      <p>Output AI Super Resolution Image</p>
    </div>
  </div>
</div>

<h5><b>Solution: Hybrid Attention Transformer (HAT)</b></h5>
<p>Image super-resolution (SR) is a classic problem in computer vision and image processing. Numerous methods based on the convolutional neural network (CNN) have been proposed and almost dominated this field in the past few years. Recently Transformers have attracted the attention of the computer vision community. Especially, a newly designed network, SwinIR, obtains a breakthrough improvement in this task.
Despite the success, “Why Transformer is better than CNN” remains a mystery. An intuitive explanation is that this kind of network can benefit from the self-attention mechanism and utilize long-range information. Interestingly, authors find that SwinIR does NOT exploit more input pixels than CNN-based methods. The proposed HAT (Hybrid Attention Transformer achieves higher pixel activation as shown below.</p>
<img height="350" src="./imgs/LAM.png" width="700">
<br><br>

<h5><b>Novel Contributions</b></h5>
<ul>
  <li>Authors have designed a novel Hybrid Attention Transformer (HAT) that combines self-attention, channel attention and a new overlapping cross-attention to activate more pixels for better reconstruction</li>
  <li>Authors propose an effective same-task pre-training strategy to further exploit the potential of SR Transformer and show the importance of large-scale data pre-training for the task</li>
  <li>Method achieves state-of-the-art performance</li>
</ul>
<img height="500" src="./imgs/Architecture.png" width="1000">
<br><br>

<h5><b>Results</b></h5>

<img height="400" src="./imgs/Result3.png" width="1000">
<img height="400" src="./imgs/Result5.png" width="1000">
<img height="400" src="./imgs/Result6.png" width="1000">
<br><br>

 
<h2>Literature Review</h2>
<p>History by Wenyu</p>
 
<h2>Biography</h2>
<p>Authors by Wenyu</p>
 
<h2>Social Impact</h2>
<p>Social Impact by Varun</p>
 
<h2>Industry Applications</h2>
<p>Industry Impact by Varun</p>
 
<h2>Follow-on Research</h2>
<p>Academic Review by Wenyu</p>
 
<h2>Peer-Review</h2>
 

<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<h3>References</h3>

<p><a name="bottou-1990">[1]</a> <a href="https://papers.baulab.info/Bottou-1990.pdf"
  >L&eacute;on Bottou and Patrick Gallinari.
  <em>A framework for the cooperation of learning algorithms.</em></a>
  Advances in neural information processing systems 3 (1990).
</p>

<h2>Team Members</h2>
                                                   
<p>Make sure to list here is who is on the team.</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
