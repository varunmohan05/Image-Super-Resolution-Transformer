<!doctype html>
<html lang="en">
<head>
<title>Image Super Resolution using Hybrid Attention Transformer</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<style>
    .image-container {
  	display: flex;
	justify-content: space-between;
      flex-wrap: wrap;
      max-width: 800px;
    }

    .image-box {
      box-sizing: border-box;
      margin-bottom: 20px;
    }

    .image-box img {
      border-radius: 8px;
    }

    .text-box {
      text-align: center;
    }

	#section1, #section2, #section3, #section4, #section5, #section6, #section7, #section8, #section9 {
      margin-bottom: 100px; 
    }
  </style>
</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Image Super Resolution using Hybrid Attention Transformer</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</h2>
</div>
</div>
<div class="row">
<div class="col">

<h2>Table of Contents</h2>
<ul>
  <li><a href="#section1">Paper Overview</a></li>
  <li><a href="#section2">Literature Review</a></li>
  <li><a href="#section3">Biography</a></li>
<li><a href="#section4">Social Impact</a></li>
	<li><a href="#section5">Industry Applications</a></li>
	<li><a href="#section6">Follow-on Research</a></li>
	<li><a href="#section7">Peer Review</a></li>
	<li><a href="#section8">Code Implementation and Experiments</a></li>
	<li><a href="#section9">References</a></li>
	
</ul>

<section id="section1">
<h2>Paper Overview</h2>
<h5><b>Problem: Image Super Resolution</b></h5>
 <p>Single image super-resolution is a classic and challenging problem in computer vision. It has practical applications in various domains, including medical imaging, satellite imagery, and enhancing the quality of digital photographs.<br>
	 We input a low-resolution image and ask the network to generate a higher-resolution image.</p>
	<div class="image-container">
  <div class="image-box">
    <img height="203" src="./imgs/LowRes.png" width="170">
    <div class="text-box">
      <p>Low-Resolution Image</p>
    </div>
  </div>
	
  <div class="image-box">
    <img height="300" src="./imgs/TraditionalInterpolation.png" width="250">
    <div class="text-box">
      <p>Traditional Interpolated Image</p>
    </div>
  </div>

  <div class="image-box">
    <img height="300" src="./imgs/SR.png" width="250">
    <div class="text-box">
      <p>AI Super Resolution Image</p>
    </div>
  </div>
</div>

<h5><b>Solution: Hybrid Attention Transformer (HAT)</b></h5>
<p> CNN-based methods have long dominated the Image super-resolution (SR) field. Recently Transformers have attracted attention. SwinIR a transformer-based model, obtained a breakthrough.
Despite the success, "Why Transformer is better than CNN" remains a mystery. An intuitive explanation is that transformers can benefit from the self-attention mechanism and utilize long-range information. Interestingly, authors find that SwinIR does NOT exploit more input pixels than CNN-based methods. The proposed HAT (Hybrid Attention Transformer achieves higher pixel activation as shown below.</p>
<img height="350" src="./imgs/LAM.png" width="700">
<br><br>

<h5><b>Novel Contributions</b></h5>
<ul>
  <li>Authors have designed a novel Hybrid Attention Transformer (HAT) that combines self-attention, channel attention and a new overlapping cross-attention to activate more pixels for better reconstruction</li>
  <li>Authors propose an effective same-task pre-training strategy to further exploit the potential of SR Transformer and show the importance of large-scale data pre-training for the task</li>
  <li>Method achieves state-of-the-art performance</li>
</ul>
<img height="500" src="./imgs/Architecture.png" width="1000">
<br><br>

<h5><b>Results</b></h5>

<img height="400" src="./imgs/Result3.png" width="1000">
<img height="400" src="./imgs/Result5.png" width="1000">
<img height="400" src="./imgs/Result6.png" width="1000">

</section>

<section id="section2">
<h2>Literature Review</h2>
<p><b>Evolution of Image Super Resolution:</b> The journey of image super-resolution (ISR) has evolved from basic image enhancement techniques to the use of advanced deep learning models. Particularly, Convolutional Neural Networks (CNNs) have played a pivotal role in addressing resolution limitations across various fields.</p>
<p><b>Transformers in Vision:</b> Building on the success in natural language processing, Transformers have been adopted into the realm of computer vision. This transition brought significant improvements in capturing long-term dependencies and processing global information in ISR tasks. In 2021, SwinIR a landmark Transformer-based network was introduced that achieved breakthrough improvement in SR. </p>
<p><b>Introduction of Local Attribution Maps (LAM):</b> The 2021 LAM paper concluded that SR networks with a wider range of involved input pixels could achieve better performance. LAM performs attribution analysis of SR networks, which aims at finding the input pixels that strongly influence the SR results.</p>
	<div class="image-box">
    <img height="350" width="900" src="./imgs/LAM_2019.png">
    <div class="text-box">
      <p>Comparison of the SR results and LAM attribution results of different SR networks. The LAM results visualize the importance
of different pixels w.r.t. the SR results</p>
    </div>
  </div>
	
<p><b>Development of HAT:</b> This paper introduces the Hybrid Attention Transformer (HAT), a novel approach that synergizes channel and self-attention mechanisms. Through the use of LAM, this development aims to overcome previous limitations of Transformers in ISR, enabling optimized utilization of input information for enhanced image reconstruction.</p>

</section>


<section id="section3">
<h2>Biography</h2>

<div class="author-bio">
  <h3>Xiangyu Chen</h3>
  <img height="100" src="https://chxy95.github.io/images/xy1.jpg" alt="Xiangyu Chen">
  <p>Xiangyu Chen, currently a joint Ph.D. student at the University of Macau and Shenzhen Institute of Advanced Technology, specializes in computer vision and computational photography with a focus on image super-resolution and general image restoration.</p>
</div>

<div class="author-bio">
  <h3>Xintao Wang</h3>
  <img height="100" src="./imgs/Xintao.png" alt="Xintao Wang">
  <p>Xintao Wang, a senior staff researcher at Tencent ARC Lab, leads efforts in visual content generation. He completed his Ph.D. at the Chinese University of Hong Kong and has made significant contributions to the field of image and video generation/editing.</p>
</div>

<div class="author-bio">
  <h3>Jiantao Zhou</h3>
  <img height="100" src="https://www.fst.um.edu.mo/image/staff-photo/jtzhou.jpg" alt="Jiantao Zhou">
  <p>Professor Jiantao Zhou, from the University of Macau, PhD in ECE, Hong Kong University of Science and Technology</p>
</div>

<div class="author-bio">
  <h3>Yu Qiao</h3>
  <img height="100" src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=gFtI-8QAAAAJ&citpid=1" alt="Yu Qiao">
  <p>Professor of Shanghai AI Laboratory; Shenzhen Institutes of Advanced Technology, Ph.D. in Mechanical Engineering @ MIT</p>
</div>

<div class="author-bio">
  <h3>Chao Dong</h3>
  <img height="100" src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=OSDCB0UAAAAJ&citpid=3" alt="Chao Dong">
  <p>Full Professor	Shenzhen Institutes of Advanced Technology	 
     PhD @ The Chinese University of Hong Kong</p>
</div>
<p>Exploring the common link,
	<ul>
		<li>Xiangyu Chen and Jiantao Zhou are with <strong>State Key Laboratory of Internet
of Things for Smart City, University of Macau.</strong></li>
	<li>Xiangyu Chen, Xiangtao Kong, Chao Dong and Yu Qiao are with <strong>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China</strong></li>
	<li>Xiangyu Chen, Wenlong Zhang, Xiangtao Kong, Chao Dong and Yu Qiao
are with <strong>Shanghai Artificial Intelligence Laboratory, Shanghai, China</strong></li>
	<li>Xintao Wang is with the <strong>Applied Research Center, Tencent PCG, Shenzhen, China</strong></li></ul>
</section>
</p>

<section id="section4">
<h2>Social Impact</h2>
<p>Since the HAT approach advances the state of the art in the Image Super-Resolution field it presents itself as the front-runner candidate to be used in Security and Surevellience products. As we know a deep learning model <strong>often hallucinates to generate additional details in the image, extra care should be taken to mitigate model biases.</strong> <br>
Similarly in the field of Medicine, care should be taken to make the Super Resolution model more reliable and only use it where hallucination is acceptable.
</p>
</section>

<section id="section5">
<h2>Industry Applications</h2>
<p>HAT promises SOTA performance and has established itself as an industry-ready model. <br>
This paper was supported by <strong>Tencent ARC lab</strong>, an active player in the super-resolution field. Their use case of the technology lies in their <strong>cloud and social media services</strong>. Cloud services benefit from super-resolution as it helps upscale low-resolution images in real-time, saving the space that a high-resolution counterpart would have otherwise occupied.<br>
Hence HAT provides companies like Tencent with an improved approach for implementing super resolution onto their products and services.
</p>
</section>

<section id="section6">
<h2>Follow-on Research</h2>
<p> This paper introduces the idea of activating more pixels to get better image reconstructions. Further research could be done to take this idea forward and produce even better-looking image reconstructions. 
<br>
A very recent paper by D. Zhang et al. named "SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution" aims to do this. They propose SwinFIR and <strong>HATFIR</strong> by replacing Fast Fourier Convolution (FFC) components, which have the image-wide receptive field leading to <strong>further improvement in the efficiency of capturing global information</strong>. 
	<div class="image-container">
  <div class="image-box">
    <img height="350" src="./imgs/HATFIR_architecture.png" width="350">
    <div class="text-box">
      <p>(a) Residual Hybrid Attention Group (RHAG) in the HAT. <br>
	      (b) Swin Fourier Transformer Block (SFTB) in HAT FIR. 
	      <br>The convolution (3x3) in RHAG of HAT is replaced 
	      <br> with SFB to get the SFTB.</p>
    </div>
  </div>
	
  <div class="image-box">
    <img height="350" src="./imgs/HATFIR_table.png" width="350">
    <div class="text-box">
      <p>Quantitative comparison with HAT</p>
    </div>
  </div>
</div>

</section>


<section id="section7">
<h2>Peer Review</h2>
<h4>Reviewer 1 - Wenyu Zhang </h4>
<div class="peer-review">
    <p><strong>Rate:</strong> 7/10</p>
    <p><strong>Strengths:</strong> Demonstrates significant improvement over existing methods, with extensive experimental validation.</p>
    <p><strong>Weaknesses:</strong> Future work could benefit from a discussion on model complexity and a broader comparative analysis.</p>
    <p><strong>Overall:</strong> Offers a substantial contribution to Transformer applications in super-resolution, meriting publication with minor revisions suggested.</p>
</div>

<h4>Reviewer 2 - Varun Mohan </h4>
<div class="peer-review">
    <p><strong>Rate:</strong> 8/10 (Strong Accept)</p>
    <p><strong>Strengths:</strong> 
	<ul>
  	<li>Achieved SOTA on several test datasets</li>
  	<li>Novel method to activate more pixels</li>
  	<li>Well-written paper and good use of figures</li>
	<li>Excellent citation of previous work</li>
	</ul></p>
    <p><strong>Weaknesses:</strong> 
	<ul>
  	<li>HAT-L variant has a considerably higher model complexity</li>
  	<li>Requires a lot of resources to pre-train and finetune the model</li>
	</ul></p>
    <p><strong>Overall:</strong> Establishes a new SOTA in Image Super-Resolution field, but fails to improve on the model complexity </p>
</div>
</section>

<section id="section8">
<h2>Code Implementation and Experiments</h2>
<h4>Experiment Idea</h4>
<p>We ran inference on 6 of the latest Super Resolution deep learning models (2 pre-trained model variations of SwinIR, HAT, SwinFIR each). Then we computed the PSNR and SSIM of the generated images and assigned them an opinion score. All the experiments produced 4x scaled images.
<br>
We have used 4 different data sets, namely Set5 [10], Set14 [11], PIRM [12], Urban [13]. In total 59 images (5 images from Set5, 14 images from Set14, first 20 images from PIRM, and first 20 images from Urban) were used to conduct each of the experiments.
<br>
PSNR, SSIM and Opinion scores were used to evaluate each super resolved image. PSNR and SSIM were computed on Y axis. Opinion scores are in the range of 1 to 10. It was assigned by taking the average of the ratings provided by both of us. The score represents the degree of resemblance between the generated image and ground truth image to a human eye. A score of 1 would mean it completely resembles the bicubic interpolated image and 10 would mean it completely resembles the ground truth image.
<br>Results are reported in Table I,II,III,V.</p>
<h4>Results</h4>
	<div class="image-container">
  <div class="image-box">
    <img height="420" src="./imgs/table1_2.png" width="400">
    
  </div>
	
  <div class="image-box">
    <img height="420" src="./imgs/table3_4.png" width="400">
  </div>
		<div class="text-box">
      <p>State-of-the-art methods performance comparison on PSNR, SSIM, and Opinion Score. HAT-L performs best and HATFIR has comparable performance.</p>
    </div>
</div>
<p>Below are some interesting result samples</p>
<div class="image-box">
    <img height="450" src="./imgs/PIRM_214.png" width="900">
    <div class="text-box">
      <p>Although all the outputs produced look similar at first glance, however on a closer look one can observe that HATFIR and HAT-L are able to distinguish between the wires better than other models. SwinFIR makes for a close second</p>
    </div>
  </div>
<div class="image-box">
    <img height="450" src="./imgs/urban_img_002.png" width="900">
    <div class="text-box">
      <p>HAT-L performs the best on this image. It is able to capture the 2 thin wires running parallel and forming a cross pattern on the glass. Whereas, other models fail to discriminate between the 2 wires and represent them as a single thick wire.</p>
    </div>
  </div>
<div class="image-box">
    <img height="450" src="./imgs/urban_img_004.png" width="900">
    <div class="text-box">
      <p>Bottom right part of the images generated by SwinIR and SwinIR Light appears to be significantly blurred. Other models capture the cube pattern better. Surprisingly, HATFIR performs better than HAT-L by eradicating the blur almost completely.</p>
    </div>
  </div>
<div class="image-box">
    <img height="450" src="./imgs/urban_img_006.png" width="900">
    <div class="text-box">
      <p>None of the models were able to super resolve the number 85. Interestingly they also failed to capture the fine cross-shaped material texture found in the ground truth image.</p>
    </div>
  </div>
<div class="image-box">
    <img height="450" src="./imgs/urban_img_016.png" width="900">
    <div class="text-box">
      <p>HAT-L and HATFIR capture the dashed line pattern on the left side of the image. Whereas other models struggle to get it right by either blurring the dashes or combining them into one.</p>
    </div>
  </div>
<div class="image-box">
    <img height="450" src="./imgs/set14_zebra.png" width="900">
    <div class="text-box">
      <p>All models perform similarly on this image. However, on a closer look at the front left leg of the zebra around the black circular patch, we notice that all models except SwinIR and SwinIR Light hallucinate an incorrect pattern of stripes (facing upwards).</p>
    </div>
  </div>
<h4>Conclusion</h4>
<p>The survey aimed to compare the best available super-resolution techniques by testing them on a fixed set of datesets (Set5, Set14, PIRM, Urban) and evaluate them not just on PSNR and SSIM, but also on MOS. After conducting the experiments we made the following observations,</p>
<ul>
	<li>HAT-L and HAT-FIR are the best-performing models. HATFIR came very close to HAT-L's performance, especially in Set5, Set14 and PIRM20 datasets</li>
	<li>HAT and HAT-L had inference times significantly greater than the rest on CPU</li>
	<li>SwinIR Light  &lt; SwinIR  &lt; HAT  &lt; SwinFIR  &lt; HATFIR  &lt; HAT-L is the general pattern that emerged with respect to the PSNR, SSIM and Opinion Scores. However with just a quick glance, it is difficult for a human eye to perceive a difference between the model outputs especially that of HAT, SwinFIR, HATFIR and HAT-L, making them all a good choice for super resolution.</li>
	<li>Accounting for model parameter count, its interesting to see HATFIR match up to HAT-L. This arguably positions HATFIR as the best super-resolution technique available.</li>
</ul>
</section>
	
<section id="section9">
<h3>References</h3>

<p><a name="Activating More Pixels in Image Super-Resolution Transformer">[1]</a> <a href="https://arxiv.org/pdf/2205.04437.pdf"
  >Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong.
  <em>"Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</em></a>
</p>
<p><a name="Super Resolution on Arm NPU">[2]</a> <a href="https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/super-resolution-on-arm-npu"
  >Alex Shang, Yabin Zheng, Mary Bennion, and Alex Avramenko.
  <em>Super Resolution on Arm NPU</em></a>
</p>
<p><a name="Super-Resolution from a Single Image">[3]</a> <a href="https://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf"
  >Daniel Glasner, Shai Bagon, Michal Irani.
  <em>"Super-Resolution from a Single Image", ICCV, 2009</em></a>
</p>
<p><a name="Learning a Deep Convolutional Network for Image Super-Resolution">[4]</a> <a href="https://arxiv.org/pdf/1501.00092.pdf"
  >Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.
  <em>"Learning a Deep Convolutional Network for Image Super-Resolution", ECCV, 2014</em></a>
</p>
<p><a name="SwinIR: Image Restoration Using Swin Transformer">[5]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf"
  >Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte.
  <em>"SwinIR: Image Restoration Using Swin Transformer", ICCV 2021</em></a>
</p>
<p><a name="SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution">[6]</a> <a href="https://arxiv.org/pdf/2208.11247.pdf"
  >Dafeng Zhang, Feiyu Huang, Shizhuo Liu, Xiaobing Wang, Zhezhu Jin.
  <em>"SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution", 2022</em></a>
</p>
<p><a name="Image super-resolution using very deep residual channel attention networks">[7]</a> <a href="https://arxiv.org/pdf/1807.02758.pdf"
  >Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
  <em>"Image super-resolution using very deep residual channel attention networks", In Proceedings of the European Conference on computer vision (ECCV), pages 286-301, 2018., ICCV 2021</em></a>
</p>
<p><a name="Interpreting super-resolution networks with local attribution maps">[8]</a> <a href="https://arxiv.org/pdf/2011.11036.pdf"
  >Jinjin Gu and Chao Dong.
  <em> "Interpreting super-resolution networks with local attribution maps", In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9199-9208, 2021.</em></a>
</p>
	  
</section>
	


<h2>Team Members</h2>
                                                   
<p>Varun Mohan (mohan.va@northeastern.edu) and Wenyu Zhang (zhang.wenyu1@northeastern.edu)</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
