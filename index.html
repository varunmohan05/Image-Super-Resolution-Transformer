<!doctype html>
<html lang="en">
<head>
<title>Image Super Resolution using Hybrid Attention Transformer</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">
<style>
    .image-container {
  	display: flex;
	justify-content: space-between;
      flex-wrap: wrap;
      max-width: 800px;
    }

    .image-box {
      box-sizing: border-box;
      margin-bottom: 20px;
    }

    .image-box img {
      border-radius: 8px;
    }

    .text-box {
      text-align: center;
    }

	#section1, #section2, #section3, #section4, #section5, #section6, #section7, #section8 {
      margin-bottom: 100px; 
    }
  </style>
</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Image Super Resolution using Hybrid Attention Transformer</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</h2>
</div>
</div>
<div class="row">
<div class="col">

<h2>Table of Contents</h2>
<ul>
  <li><a href="#section1">Paper Overview</a></li>
  <li><a href="#section2">Literature Review</a></li>
  <li><a href="#section3">Biography</a></li>
<li><a href="#section4">Social Impact</a></li>
	<li><a href="#section5">Industry Applications</a></li>
	<li><a href="#section6">Follow-on Research</a></li>
	<li><a href="#section7">Peer Review</a></li>
	<li><a href="#section8">References</a></li>
</ul>

<section id="section1">
<h2>Paper Overview</h2>
<h5><b>Problem: Image Super Resolution</b></h5>
 <p>Single image super-resolution is a classic and challenging problem in computer vision. It has practical applications in various domains, including medical imaging, satellite imagery, and enhancing the quality of digital photographs.<br>
	 We input a low-resolution image and ask the network to generate a higher-resolution image.</p>
	<div class="image-container">
  <div class="image-box">
    <img height="203" src="./imgs/LowRes.png" width="170">
    <div class="text-box">
      <p>Low-Resolution Image</p>
    </div>
  </div>
	
  <div class="image-box">
    <img height="300" src="./imgs/TraditionalInterpolation.png" width="250">
    <div class="text-box">
      <p>Traditional Interpolated Image</p>
    </div>
  </div>

  <div class="image-box">
    <img height="300" src="./imgs/SR.png" width="250">
    <div class="text-box">
      <p>AI Super Resolution Image</p>
    </div>
  </div>
</div>

<h5><b>Solution: Hybrid Attention Transformer (HAT)</b></h5>
<p> CNN-based methods have long dominated the Image super-resolution (SR) field. Recently Transformers have attracted attention. SwinIR a transformer-based model, obtained a breakthrough.
Despite the success, "Why Transformer is better than CNN" remains a mystery. An intuitive explanation is that transformers can benefit from the self-attention mechanism and utilize long-range information. Interestingly, authors find that SwinIR does NOT exploit more input pixels than CNN-based methods. The proposed HAT (Hybrid Attention Transformer achieves higher pixel activation as shown below.</p>
<img height="350" src="./imgs/LAM.png" width="700">
<br><br>

<h5><b>Novel Contributions</b></h5>
<ul>
  <li>Authors have designed a novel Hybrid Attention Transformer (HAT) that combines self-attention, channel attention and a new overlapping cross-attention to activate more pixels for better reconstruction</li>
  <li>Authors propose an effective same-task pre-training strategy to further exploit the potential of SR Transformer and show the importance of large-scale data pre-training for the task</li>
  <li>Method achieves state-of-the-art performance</li>
</ul>
<img height="500" src="./imgs/Architecture.png" width="1000">
<br><br>

<h5><b>Results</b></h5>

<img height="400" src="./imgs/Result3.png" width="1000">
<img height="400" src="./imgs/Result5.png" width="1000">
<img height="400" src="./imgs/Result6.png" width="1000">

</section>

<section id="section2">
<h2>Literature Review</h2>
<p>History by Wenyu</p>
</section>

<section id="section3">
<h2>Biography</h2>
<p>Authors by Wenyu</p>
</section>

<section id="section4">
<h2>Social Impact</h2>
<p>Social Impact by Varun</p>
</section>

<section id="section5">
<h2>Industry Applications</h2>
<p>Industry Impact by Varun</p>
</section>

<section id="section6">
<h2>Follow-on Research</h2>
<p>Academic Review by Wenyu</p>
</section>

<section id="section7">
<h2>Peer-Review</h2>
</section>

<section id="section8">
<h3>References</h3>

<p><a name="Activating More Pixels in Image Super-Resolution Transformer">[1]</a> <a href="https://arxiv.org/pdf/2205.04437.pdf"
  >Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong.
  <em>"Activating More Pixels in Image Super-Resolution Transformer", CVPR 2023</em></a>
</p>
<p><a name="Super Resolution on Arm NPU">[2]</a> <a href="https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/super-resolution-on-arm-npu"
  >Alex Shang, Yabin Zheng, Mary Bennion, and Alex Avramenko.
  <em>Super Resolution on Arm NPU</em></a>
</p>
<p><a name="Super-Resolution from a Single Image">[3]</a> <a href="https://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf"
  >Daniel Glasner, Shai Bagon, Michal Irani.
  <em>"Super-Resolution from a Single Image", ICCV, 2009</em></a>
</p>
<p><a name="Learning a Deep Convolutional Network for Image Super-Resolution">[4]</a> <a href="https://arxiv.org/pdf/1501.00092.pdf"
  >Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang.
  <em>"Learning a Deep Convolutional Network for Image Super-Resolution", ECCV, 2014</em></a>
</p>
<p><a name="SwinIR: Image Restoration Using Swin Transformer">[5]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf"
  >Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte.
  <em>"SwinIR: Image Restoration Using Swin Transformer", ICCV 2021</em></a>
</p>
</section>
	


<h2>Team Members</h2>
                                                   
<p>Varun Mohan (mohan.va@northeastern.edu) and Wenyu Zhang (zhang.wenyu1@northeastern.edu)</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
